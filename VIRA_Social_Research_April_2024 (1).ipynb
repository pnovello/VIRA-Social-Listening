{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4c_Y7syY9oX"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNIPTSK-47K4"
      },
      "outputs": [],
      "source": [
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU37E1MFbRNz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn2GO0PYYvh6"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWjiEKKkbRNz"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.executable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG26asv7bRNz"
      },
      "outputs": [],
      "source": [
        "!{sys.executable} -m pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGBSgYznbRNz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from numpy import nan\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from langdetect import detect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8Ka1N4obRNz"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "df_may23 = pd.read_csv(\"vira_logs_2023-08-30.csv\")\n",
        "\n",
        "# Convert 'date' column to datetime format\n",
        "df_may23['date'] = pd.to_datetime(df_may23['date'], errors='coerce')\n",
        "\n",
        "start_date = \"2021-07-01\"\n",
        "end_date = \"2023-05-31\"\n",
        "mask = (df_may23['date'] >= start_date) & (df_may23['date'] <= end_date)\n",
        "df_may23 = df_may23[mask]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFuHMnGz2l-T"
      },
      "outputs": [],
      "source": [
        "df_may23.to_csv('filtered_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHtgi7YSbRNz"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('filtered_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGv6Gd_MbRN0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert the 'date' column to datetime format\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Filter only 'user' messages\n",
        "user_msgs = df[df['side'] == 'user']\n",
        "\n",
        "# Create a separate dataframe with only the dates of user messages\n",
        "dates_df = user_msgs[['date']].copy()\n",
        "\n",
        "# Extract year and month information from the date\n",
        "dates_df['Year'] = dates_df['date'].dt.year\n",
        "dates_df['Month'] = dates_df['date'].dt.month\n",
        "\n",
        "# Group by year and month to get the count of messages\n",
        "quarterly_counts = dates_df.groupby(['Year', 'Month']).size().reset_index(name='Message Count')\n",
        "\n",
        "print(quarterly_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyQSxsnLbRN0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert the 'date' column to datetime format\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Filter only 'user' messages\n",
        "user_msgs = df[df['side'] == 'user']\n",
        "\n",
        "# Count the total number of user messages\n",
        "total_user_msgs_count = len(user_msgs)\n",
        "\n",
        "print(f\"Total 'user' messages count: {total_user_msgs_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJRseru9ssOf"
      },
      "outputs": [],
      "source": [
        "for index, row in df.iterrows():\n",
        "    # Check if the value in the column 'column_name' is either NaN or not a string\n",
        "    if pd.isna(row['text']) or not isinstance(row['text'], str):\n",
        "        # Drop the row if the condition is met\n",
        "        df.drop(index, inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nmqUEnh2t7B"
      },
      "outputs": [],
      "source": [
        "df.drop(df[(df.is_concern==False) & (df.dialog_act.str.contains(\"FEEDBACK\"))].index, inplace=True)\n",
        "df.drop(df[df.text.str.contains(\"None of the above\")].index, inplace=True)\n",
        "df.drop(df[df.text.str.contains(\"I didn't ask a question\")].index, inplace=True)\n",
        "df.drop(df[df.text.str.contains(\"Ninguna de las anteriores\")].index, inplace=True)\n",
        "df.drop(df[df.text.str.contains(\"Ninguna de las anteriores\")].index, inplace=True)\n",
        "df.drop(df[df.text.str.contains(\"No hice ninguna pregunta\")].index, inplace=True)\n",
        "df.drop(df[df.is_concern!=True].index, inplace=True)\n",
        "df.drop(df[df.is_profanity == \"TRUE\"].index, inplace=True)\n",
        "df.drop(df[df.date == \"2021-07-28\"].index, inplace=True)\n",
        "df.drop(df[df.date == \"2021-07-29\"].index, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyWc1k36HPjC"
      },
      "outputs": [],
      "source": [
        "# Convert the 'date' column to datetime format\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Filter only 'user' messages\n",
        "user_msgs = df[df['side'] == 'user']\n",
        "\n",
        "# Count the total number of user messages\n",
        "total_user_msgs_count = len(user_msgs)\n",
        "\n",
        "print(f\"Total 'user' messages count: {total_user_msgs_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lftbs12qU-Ld"
      },
      "outputs": [],
      "source": [
        "df_text = df[['text']]\n",
        "docs_unclean = df_text.text.tolist()\n",
        "timestamps_unclean = df.date.to_list()\n",
        "docs = []\n",
        "timestamps = []\n",
        "index = 0\n",
        "for item in docs_unclean:\n",
        "  try:\n",
        "    lang = detect(item)\n",
        "    language_list = ['en', 'es']\n",
        "    if str(item) != 'nan' and type(item) == str and lang in language_list:\n",
        "      docs.append(item)\n",
        "      timestamps.append(timestamps_unclean[index])\n",
        "    index = index + 1\n",
        "  except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjYc0EzqbRN0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a DataFrame from the preprocessed documents and their timestamps\n",
        "preprocessed_data = pd.DataFrame({\n",
        "    'text': docs,\n",
        "    'date': timestamps\n",
        "})\n",
        "\n",
        "# Save the preprocessed data to a CSV file\n",
        "preprocessed_data.to_csv(\"preprocessed_data.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnlGg95EbRN0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "\n",
        "# Load the preprocessed data from the CSV file\n",
        "preprocessed_data = pd.read_csv(\"preprocessed_data.csv\")\n",
        "\n",
        "# Extract the documents as a list\n",
        "docs = preprocessed_data['text'].tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize the CountVectorizer with English stop words\n",
        "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
        "\n",
        "# Initialize the BERTopic model with the specified parameters\n",
        "#topic_model = BERTopic(embedding_model=\"all-MiniLM-L6-v2\",\n",
        "  #                     nr_topics=\"auto\",\n",
        " #                      language=\"multilingual\",\n",
        " #                      vectorizer_model=vectorizer_model,\n",
        " #                      calculate_probabilities=False)\n",
        "# Trying new methods of number of topics\n",
        "\n",
        "\n",
        "\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=20, metric='euclidean',\n",
        "                      cluster_selection_method='eom', prediction_data=True, min_samples=10)\n",
        "\n",
        "\n",
        "topic_model = BERTopic(embedding_model=\"all-MiniLM-L6-v2\",hdbscan_model = hdbscan_model,\n",
        "                       nr_topics=75,\n",
        "                       language=\"multilingual\",\n",
        "                       vectorizer_model=vectorizer_model,\n",
        "                       calculate_probabilities=False)\n",
        "\n",
        "\n",
        "# Fit the BERTopic model to the documents and get the topics and probabilities\n",
        "topics, probs = topic_model.fit_transform(docs)\n",
        "\n",
        "# Retrieve the topic information\n",
        "topic_info = topic_model.get_topic_info()\n",
        "\n",
        "topic_info.to_csv(\"vira_may23_topic_info_results.csv\", index=False)\n",
        "# You may want to save the BERTopic model after fitting for future use\n",
        "topic_model.save(\"my_bertopic_model\")\n"
      ],
      "metadata": {
        "id": "bWYzkBD5J5_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRenOkkYsa2T"
      },
      "outputs": [],
      "source": [
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T = topic_model.get_document_info(docs)\n",
        "T.to_csv(\"vira_may23_topic_info_results_docXdoc.csv\", index=False)"
      ],
      "metadata": {
        "id": "yTH_TyquRnNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPcKoq4Dic9y"
      },
      "outputs": [],
      "source": [
        "# Trying to fix topic clarity by enforcing min topic size\n",
        "\n",
        "new_topic_model = BERTopic(embedding_model=\"all-MiniLM-L6-v2\",\n",
        "                       nr_topics=100,\n",
        "                       language=\"multilingual\",\n",
        "                       vectorizer_model=vectorizer_model,\n",
        "                       min_topic_size = 20,\n",
        "                       calculate_probabilities=False)\n",
        "\n",
        "topics, probs = new_topic_model.fit_transform(docs)\n",
        "topic_info = new_topic_model.get_topic_info()\n",
        "topic_info.to_csv(\"vira_may23_topic_info_results_updated.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ_sWaESbRN0"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaS3yIh4i0o-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cSHRWAgbnkg"
      },
      "outputs": [],
      "source": [
        "topic_df = pd.DataFrame({\"topic\": topics, \"document\": docs})\n",
        "topic_df.drop(topic_df[topic_df.topic == -1].index, inplace=True)\n",
        "\n",
        "topic_rep_dict = {}\n",
        "for i in range(50):\n",
        "  topic_rep_dict[i] = topic_model.get_representative_docs(i)\n",
        "\n",
        "topic_data_df = pd.DataFrame.from_dict(topic_rep_dict)\n",
        "topic_data_df.to_csv('vira_may23_topic_rep_docs_removed_dates_toxic_lang_detect_updated.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ov1-5HAnpvyE"
      },
      "outputs": [],
      "source": [
        "!pip install fuzzywuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "mldpf2LxEs83"
      },
      "outputs": [],
      "source": [
        "pip install bottleneck --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "PJDOLE8QEs83"
      },
      "outputs": [],
      "source": [
        "from fuzzywuzzy import fuzz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnwLwRckEs83"
      },
      "source": [
        "## Mapping to Topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "YfxWqWdREs84"
      },
      "outputs": [],
      "source": [
        "from fuzzywuzzy import process\n",
        "\n",
        "# Load your original data (make sure to replace 'your_original_data.csv' with the actual file path)\n",
        "original_data = pd.read_csv('preprocessed_data.csv')\n",
        "\n",
        "# Assuming topic_keywords is a dictionary where the key is the topic number and the value is the list of keywords\n",
        "# Replace 'your_topics_data.csv' with the actual file path\n",
        "topics_data = pd.read_csv('vira_may23_topic_info_results_75_topics.csv')\n",
        "topic_keywords = {row['Topic']: row['Representation'].strip(\"[]\").replace(\"'\", \"\").split(', ') for index, row in topics_data.iterrows()}\n",
        "\n",
        "# Define a function to match text to topics using fuzzy matching\n",
        "def match_text_to_topics(text, topic_keywords):\n",
        "    # Find the best match for the text from the topic keywords\n",
        "    best_match, highest_score = None, 0\n",
        "    for topic, keywords in topic_keywords.items():\n",
        "        # Join the keywords into a single string for comparison\n",
        "        keywords_string = ' '.join(keywords)\n",
        "        score = process.extractOne(text, [keywords_string], scorer=fuzz.token_sort_ratio)[1]\n",
        "        if score > highest_score:\n",
        "            best_match, highest_score = topic, score\n",
        "    return best_match\n",
        "\n",
        "# Apply the function to each row in the original data DataFrame\n",
        "original_data['Topic'] = original_data['text'].apply(lambda x: match_text_to_topics(x, topic_keywords))\n",
        "\n",
        "# Save the labeled original data\n",
        "original_data.to_csv('labeled_original_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5oWBMLpprl6"
      },
      "outputs": [],
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "def find_best_theme(topic_keywords, theme_keywords, similarity_threshold):\n",
        "    best_match = None\n",
        "    max_similarity = -1\n",
        "\n",
        "    for theme, keywords in theme_keywords.items():\n",
        "        for keyword in keywords:\n",
        "            similarity = fuzz.token_set_ratio(topic_keywords, keyword)\n",
        "            if similarity > max_similarity:\n",
        "                max_similarity = similarity\n",
        "                best_match = theme\n",
        "\n",
        "    if max_similarity >= similarity_threshold:\n",
        "        return best_match\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Can add a similarity threshold to see which topics belong to multiple themes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3mT5mJF2KUr"
      },
      "outputs": [],
      "source": [
        "# combined\n",
        "themes = {\n",
        "    \"Vaccine Information and Safety\": [\n",
        "        \"vaccine\",\n",
        "        \"vaccines\",\n",
        "        \"vaccinate\"\n",
        "        \"safe\",\n",
        "        \"safety\",\n",
        "        \"efficacy\",\n",
        "        \"side effects\",\n",
        "        \"development\",\n",
        "        \"immunization\",\n",
        "        \"adverse events\",\n",
        "        \"vaccination\",\n",
        "        \"doses\",\n",
        "        \"booster\",\n",
        "        \"shot\",\n",
        "        \"boosted\",\n",
        "        \"immunity\",\n",
        "        \"bivalent\",\n",
        "        \"deaths\",\n",
        "        \"pfizer\",\n",
        "        \"moderna\",\n",
        "        \"novavax\",\n",
        "        \"j&j\",\n",
        "        \"johnson and johnson\",\n",
        "        \"astrazeneca\",\n",
        "        \"myocarditis\",\n",
        "        \"fda\",\n",
        "        \"microchip\",\n",
        "        \"children\",\n",
        "        \"jansen\",\n",
        "        \"appointment\",\n",
        "        \"effective\",\n",
        "        \"hurt\",\n",
        "        \"cost\",\n",
        "        \"protect\",\n",
        "        \"dying\",\n",
        "        \"died\",\n",
        "        \"emergency use\",\n",
        "        \"technology\",\"trials\",\n",
        "        \"ingredients\",\n",
        "        \"JNJ\",\n",
        "        \"breastfeeding\",\n",
        "        \"fertility\",\n",
        "        \"register\",\n",
        "        \"pre-existing\",\n",
        "        \"card\",\n",
        "        \"pharma\",\n",
        "        \"by-products\",\n",
        "        \"test\",\n",
        "        \"diagnosis\",\n",
        "        \"PCR\",\n",
        "        \"antigen\",\n",
        "        \"accuracy\",\n",
        "        \"results\",\n",
        "        \"testing sites\",\n",
        "        \"laboratory\",\n",
        "        \"rapid\",\n",
        "        \"swab\",\n",
        "        \"at-home\",\n",
        "        \"positive\",\n",
        "        \"symptoms\",\n",
        "        \"transmission\",\n",
        "        \"variants\",\n",
        "        \"outbreak\",\n",
        "        \"infection\",\n",
        "        \"pandemic\",\n",
        "        \"spread\",\n",
        "        \"incubation\",\n",
        "        \"asymptomatic\",\n",
        "        \"symptomatic\",\n",
        "        \"reinfection\",\n",
        "        \"hospitalization\",\n",
        "        \"omicron\",\n",
        "        \"delta\",\n",
        "        \"mRNA\",\n",
        "        \"spike protein\",\n",
        "        \"breakthrough\",\n",
        "        \"mortality\",\n",
        "        \"long COVID\",\n",
        "        \"covid\",\n",
        "        \"pediatric\",\n",
        "        \"ivermectin\",\n",
        "        \"antibodies\",\n",
        "        \"mutations\",\n",
        "        \"mutate\",\n",
        "        \"Sars-Cov-2\",\n",
        "        \"novel\",\n",
        "        \"flu\"\n",
        "    ],\n",
        "    \"Community and trust\": [\n",
        "        \"hesitancy\",\n",
        "        \"community\",\n",
        "        \"communication\",\n",
        "        \"hoax\",\n",
        "        \"trust\",\n",
        "        \"distribution\",\n",
        "        \"engagement\",\n",
        "        \"vulnerable\",\n",
        "        \"equity\",\n",
        "        \"vaccinators\",\n",
        "        \"misinformation\",\n",
        "        \"resistance\",\n",
        "        \"disparities\",\n",
        "        \"fake\",\n",
        "        \"social\",\n",
        "        \"religion\",\n",
        "        \"marketing\",\n",
        "        \"facebook\",\n",
        "        \"whatsapp\",\n",
        "        \"benefits\",\n",
        "        \"chatbot\",\n",
        "        \"clinic\",\n",
        "        \"liability\",\n",
        "        \"VIRA\",\n",
        "        \"convince\",\n",
        "        \"guidelines\",\n",
        "        \"precautions\",\n",
        "        \"distancing\",\n",
        "        \"masks\",\n",
        "        \"hygiene\",\n",
        "        \"quarantine\",\n",
        "        \"isolation\",\n",
        "        \"ventilation\",\n",
        "        \"gathering\",\n",
        "        \"travel\",\n",
        "        \"exposure\",\n",
        "        \"school\",\n",
        "        \"work\",\n",
        "        \"pregnant\",\n",
        "        \"children\",\n",
        "        \"mandate\",\n",
        "        \"restrictions\"\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TTvk8e5bRN1"
      },
      "outputs": [],
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "import pandas as pd\n",
        "\n",
        "def find_best_theme(message, theme_keywords, similarity_threshold=50):\n",
        "    best_match_theme = None\n",
        "    max_similarity = -1\n",
        "\n",
        "    for theme, keywords in theme_keywords.items():\n",
        "        for keyword in keywords:\n",
        "            similarity = fuzz.token_set_ratio(str(message).lower(), keyword.lower())\n",
        "            if similarity > max_similarity:\n",
        "                max_similarity = similarity\n",
        "                best_match_theme = theme\n",
        "\n",
        "    if max_similarity >= similarity_threshold:\n",
        "        return best_match_theme\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Convert 'date' column to datetime format\n",
        "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "\n",
        "# Assign themes to each message using the `find_best_theme` function\n",
        "df['theme'] = df['text'].apply(lambda message: find_best_theme(message, themes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dIe1ysabRN1"
      },
      "outputs": [],
      "source": [
        "# Group the data by quarter and theme, then compute counts\n",
        "quarterly_counts = df.groupby([pd.Grouper(key='date', freq='M'), 'theme']).size().unstack().fillna(0)\n",
        "\n",
        "# Save to CSV\n",
        "quarterly_counts.to_csv('monthly_theme_counts.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATA7mudNbRN1"
      },
      "outputs": [],
      "source": [
        "# convert from wide to long - prep for Tableau\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "wide_df = pd.read_csv('monthly_theme_counts.csv')\n",
        "\n",
        "# Convert to long format\n",
        "long_df = pd.melt(wide_df, id_vars=['date'], value_vars=wide_df.columns[1:])\n",
        "\n",
        "# Save to a new csv if needed\n",
        "long_df.to_csv('monthly_theme_counts_long_format.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qM_gtdVgbRN1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Load the data\n",
        "dq = pd.read_csv('quarterly_theme_counts.csv')\n",
        "\n",
        "# Set up lists to hold data for the Sankey diagram\n",
        "source = []\n",
        "target = []\n",
        "value = []\n",
        "label = []\n",
        "color = []\n",
        "link_colors = []  # list to hold colors for the links\n",
        "\n",
        "# Define colors for themes and dates\n",
        "theme_colors = [\"blue\", \"green\", \"red\", \"yellow\", \"purple\"]\n",
        "date_colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\"]  # colors for dates\n",
        "\n",
        "# Assuming the first column in dq is 'date', and other columns are theme names\n",
        "for i, date in enumerate(dq['date']):\n",
        "    for j, theme in enumerate(dq.columns[1:]):  # skipping the 'date' column\n",
        "        source.append(i)\n",
        "        target.append(len(dq['date']) + j)\n",
        "        value.append(dq.loc[i, theme])\n",
        "        link_colors.append(theme_colors[j])\n",
        "    label.append(date)\n",
        "    color.append(date_colors[i % len(date_colors)])  # use modulo to ensure we don't go out of range\n",
        "\n",
        "# Adding theme names to the labels and their colors\n",
        "label.extend(dq.columns[1:])\n",
        "color.extend(theme_colors)\n",
        "\n",
        "# Bold the date labels\n",
        "label = ['<b>' + lbl + '</b>' if lbl in dq['date'].values else lbl for lbl in label]\n",
        "\n",
        "# Create the Sankey diagram\n",
        "fig = go.Figure(go.Sankey(\n",
        "    node=dict(pad=15, thickness=20, line=dict(color=\"black\", width=0.5), label=label, color=color),\n",
        "    link=dict(source=source, target=target, value=value, color=link_colors)  # adding link colors here\n",
        "))\n",
        "\n",
        "# Set the title and display the diagram\n",
        "fig.update_layout(title_text=\"Quarterly Theme Counts Sankey Diagram\", font_size=10)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If2SsNOkbRN4"
      },
      "outputs": [],
      "source": [
        "theme_keywords = themes  # Dictionary of themes and their keywords\n",
        "similarity_threshold = 50  # Adjust this threshold based on your needs\n",
        "topic_keywords = topic_model.get_topics()\n",
        "results = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leJO_rpupruG"
      },
      "outputs": [],
      "source": [
        "# Iterate over each topic\n",
        "for topic_id, topic_keywords in topic_keywords.items():\n",
        "    if topic_id >= 0:\n",
        "        # Find the best matching theme for the current topic\n",
        "        best_theme = find_best_theme(topic_keywords, theme_keywords, similarity_threshold)\n",
        "        # Store the topic and its corresponding theme in a list\n",
        "        topic_description = topic_model.get_representative_docs(topic_id)[0]\n",
        "        results.append({'Topic': topic_description, 'Theme': best_theme})\n",
        "\n",
        "# Create a DataFrame from the results\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv('prelim_topic_data_theme_threshold_lang_detect_mecklenburg.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wcsjv5dBprxB"
      },
      "outputs": [],
      "source": [
        "similarity = fuzz.token_set_ratio(['is', 'vaccine', 's'], \"safety\")\n",
        "print(similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gyJ2nqtbRN5"
      },
      "outputs": [],
      "source": [
        "#assign only one theme to one topic:\n",
        "from fuzzywuzzy import fuzz\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "def find_best_theme(topic_keywords, theme_keywords, similarity_threshold):\n",
        "    best_match = None\n",
        "    max_similarity = -1\n",
        "\n",
        "    for theme, keywords in theme_keywords.items():\n",
        "        for keyword in keywords:\n",
        "            similarity = fuzz.token_set_ratio(topic_keywords, keyword)\n",
        "            if similarity > max_similarity:\n",
        "                max_similarity = similarity\n",
        "                best_match = theme\n",
        "\n",
        "    if max_similarity >= similarity_threshold:\n",
        "        return best_match\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "theme_keywords = themes  # Dictionary of themes and their keywords\n",
        "similarity_threshold = 50  # Adjust this threshold based on your needs\n",
        "topic_keywords = topic_model.get_topics()\n",
        "results = []\n",
        "\n",
        "# Iterate over each topic\n",
        "for topic_id, topic_keywords in topic_keywords.items():\n",
        "    if topic_id >= 0:\n",
        "        # Find the best matching theme for the current topic\n",
        "        best_theme = find_best_theme(topic_keywords, theme_keywords, similarity_threshold)\n",
        "        # Store the topic and its corresponding theme in a list\n",
        "        topic_description = topic_model.get_representative_docs(topic_id)[0]\n",
        "        results.append({'Topic': topic_description, 'Theme': best_theme})\n",
        "\n",
        "# Create a DataFrame from the results\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv('up_prelim_topic_data_theme_threshold_lang_detect_mecklenburg.csv')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}